{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed441ca-68aa-4045-88c3-aea34a95aa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/19 08:27:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .appName(\"notebook-test\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.cores.max\", \"1\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a340db-3b02-4054-9770-c3b6064160a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark://spark-master:7077'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d14cc5e-2ef3-4bac-9e90-575b3c50b1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1_000_000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d05664df-1a8c-4e57-9741-73ec003d5aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://2acfba0f18d0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>notebook-test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x72254032f670>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkSession.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6648e5b1-3480-45b7-b822-feb540849547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|cleaned  |\n",
      "|default  |\n",
      "|features |\n",
      "|graph    |\n",
      "|marts    |\n",
      "|raw      |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " spark.sql(\"SHOW NAMESPACES IN hive_cat\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d8a62f4-4b80-420e-9e36-5e25c9906b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320670ac-034d-4184-b361-5e7d8f7f8950",
   "metadata": {},
   "source": [
    "# Создание слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2872d0-7024-43fd-8159-f72e3f9d53c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|catalog      |\n",
      "+-------------+\n",
      "|hive_cat     |\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b41f97b4-2106-421f-9e6b-360dccccda85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|cleaned  |\n",
      "|default  |\n",
      "|features |\n",
      "|graph    |\n",
      "|marts    |\n",
      "|raw      |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " spark.sql(\"SHOW NAMESPACES IN hive_cat\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7b1753-aa21-489a-a109-876af563d3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.sql(\"create NAMESPACE if not exists hive_cat.raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9361f9-95a5-44f1-8c2d-4dab7aca0f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.sql(\"create NAMESPACE if not exists hive_cat.cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2311d0d5-0104-4471-8930-c1cc6afca9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.sql(\"create NAMESPACE if not exists hive_cat.features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6835de75-bb70-4f28-8f58-71c51b95419b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.sql(\"create NAMESPACE if not exists hive_cat.marts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78532df4-8984-4050-a770-517367b48b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.sql(\"create NAMESPACE if not exists hive_cat.graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c21619-fd75-421d-b140-c4466189c62b",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0978b338-9a90-43ce-ab2c-52f9d4175e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_to_iceberg(csv_path, table_name):\n",
    "    #print(f\"Reading CSV from: {csv_path}\")\n",
    "    \n",
    "    df_csv = (spark.read\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"inferSchema\", \"true\")\n",
    "              .csv(csv_path)\n",
    "    )\n",
    "    \n",
    "    #print(f\"Writing to Iceberg table: {table_name}\")\n",
    "    \n",
    "    (df_csv.writeTo(table_name)\n",
    "     .using(\"iceberg\")\n",
    "     .createOrReplace()\n",
    "    )\n",
    "    \n",
    "    #print(f\"First 5 rows from {table_name}:\")\n",
    "    #spark.table(table_name).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5fb8ad-7d67-4969-a570-7b14543fca7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 08:10:17 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:10:32 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:10:47 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:11:02 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:11:17 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:11:32 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:11:47 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:12:02 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:12:17 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:12:32 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:12:47 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:13:02 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:13:17 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:13:32 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/19 08:13:46 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/12/19 08:13:46 ERROR Utils: Uncaught exception in thread stop-spark-context\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient.stop(StandaloneAppClient.scala:288)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:275)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:142)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop$(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stop(CoarseGrainedSchedulerBackend.scala:54)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2203)\n",
      "Caused by: org.apache.spark.SparkException: Could not find AppClient.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:72)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o63.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_csv_to_iceberg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/test_transaction.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhive_cat.raw.test_transaction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mprocess_csv_to_iceberg\u001b[0;34m(csv_path, table_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_csv_to_iceberg\u001b[39m(csv_path, table_name):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#print(f\"Reading CSV from: {csv_path}\")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     df_csv \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#print(f\"Writing to Iceberg table: {table_name}\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     (df_csv\u001b[38;5;241m.\u001b[39mwriteTo(table_name)\n\u001b[1;32m     13\u001b[0m      \u001b[38;5;241m.\u001b[39musing(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miceberg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m      \u001b[38;5;241m.\u001b[39mcreateOrReplace()\n\u001b[1;32m     15\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o63.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "process_csv_to_iceberg(\"/data/test_transaction.csv\", \"hive_cat.raw.test_transaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96605e50-45b9-47eb-8139-b3f25100d5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[TransactionID: int, id-01: double, id-02: double, id-03: double, id-04: double, id-05: double, id-06: double, id-07: double, id-08: double, id-09: double, id-10: double, id-11: double, id-12: string, id-13: double, id-14: double, id-15: string, id-16: string, id-17: double, id-18: double, id-19: double, id-20: double, id-21: double, id-22: double, id-23: string, id-24: double, id-25: double, id-26: double, id-27: string, id-28: string, id-29: string, id-30: string, id-31: string, id-32: double, id-33: string, id-34: string, id-35: string, id-36: string, id-37: string, id-38: string, DeviceType: string, DeviceInfo: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_csv_to_iceberg(\"/data/test_identity.csv\", \"hive_cat.raw.test_identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5262b6ca-6984-461c-8254-1106e072837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[TransactionID: int, isFraud: int, TransactionDT: int, TransactionAmt: double, ProductCD: string, card1: int, card2: double, card3: double, card4: string, card5: double, card6: string, addr1: double, addr2: double, dist1: double, dist2: double, P_emaildomain: string, R_emaildomain: string, C1: double, C2: double, C3: double, C4: double, C5: double, C6: double, C7: double, C8: double, C9: double, C10: double, C11: double, C12: double, C13: double, C14: double, D1: double, D2: double, D3: double, D4: double, D5: double, D6: double, D7: double, D8: double, D9: double, D10: double, D11: double, D12: double, D13: double, D14: double, D15: double, M1: string, M2: string, M3: string, M4: string, M5: string, M6: string, M7: string, M8: string, M9: string, V1: double, V2: double, V3: double, V4: double, V5: double, V6: double, V7: double, V8: double, V9: double, V10: double, V11: double, V12: double, V13: double, V14: double, V15: double, V16: double, V17: double, V18: double, V19: double, V20: double, V21: double, V22: double, V23: double, V24: double, V25: double, V26: double, V27: double, V28: double, V29: double, V30: double, V31: double, V32: double, V33: double, V34: double, V35: double, V36: double, V37: double, V38: double, V39: double, V40: double, V41: double, V42: double, V43: double, V44: double, V45: double, V46: double, V47: double, V48: double, V49: double, V50: double, V51: double, V52: double, V53: double, V54: double, V55: double, V56: double, V57: double, V58: double, V59: double, V60: double, V61: double, V62: double, V63: double, V64: double, V65: double, V66: double, V67: double, V68: double, V69: double, V70: double, V71: double, V72: double, V73: double, V74: double, V75: double, V76: double, V77: double, V78: double, V79: double, V80: double, V81: double, V82: double, V83: double, V84: double, V85: double, V86: double, V87: double, V88: double, V89: double, V90: double, V91: double, V92: double, V93: double, V94: double, V95: double, V96: double, V97: double, V98: double, V99: double, V100: double, V101: double, V102: double, V103: double, V104: double, V105: double, V106: double, V107: double, V108: double, V109: double, V110: double, V111: double, V112: double, V113: double, V114: double, V115: double, V116: double, V117: double, V118: double, V119: double, V120: double, V121: double, V122: double, V123: double, V124: double, V125: double, V126: double, V127: double, V128: double, V129: double, V130: double, V131: double, V132: double, V133: double, V134: double, V135: double, V136: double, V137: double, V138: double, V139: double, V140: double, V141: double, V142: double, V143: double, V144: double, V145: double, V146: double, V147: double, V148: double, V149: double, V150: double, V151: double, V152: double, V153: double, V154: double, V155: double, V156: double, V157: double, V158: double, V159: double, V160: double, V161: double, V162: double, V163: double, V164: double, V165: double, V166: double, V167: double, V168: double, V169: double, V170: double, V171: double, V172: double, V173: double, V174: double, V175: double, V176: double, V177: double, V178: double, V179: double, V180: double, V181: double, V182: double, V183: double, V184: double, V185: double, V186: double, V187: double, V188: double, V189: double, V190: double, V191: double, V192: double, V193: double, V194: double, V195: double, V196: double, V197: double, V198: double, V199: double, V200: double, V201: double, V202: double, V203: double, V204: double, V205: double, V206: double, V207: double, V208: double, V209: double, V210: double, V211: double, V212: double, V213: double, V214: double, V215: double, V216: double, V217: double, V218: double, V219: double, V220: double, V221: double, V222: double, V223: double, V224: double, V225: double, V226: double, V227: double, V228: double, V229: double, V230: double, V231: double, V232: double, V233: double, V234: double, V235: double, V236: double, V237: double, V238: double, V239: double, V240: double, V241: double, V242: double, V243: double, V244: double, V245: double, V246: double, V247: double, V248: double, V249: double, V250: double, V251: double, V252: double, V253: double, V254: double, V255: double, V256: double, V257: double, V258: double, V259: double, V260: double, V261: double, V262: double, V263: double, V264: double, V265: double, V266: double, V267: double, V268: double, V269: double, V270: double, V271: double, V272: double, V273: double, V274: double, V275: double, V276: double, V277: double, V278: double, V279: double, V280: double, V281: double, V282: double, V283: double, V284: double, V285: double, V286: double, V287: double, V288: double, V289: double, V290: double, V291: double, V292: double, V293: double, V294: double, V295: double, V296: double, V297: double, V298: double, V299: double, V300: double, V301: double, V302: double, V303: double, V304: double, V305: double, V306: double, V307: double, V308: double, V309: double, V310: double, V311: double, V312: double, V313: double, V314: double, V315: double, V316: double, V317: double, V318: double, V319: double, V320: double, V321: double, V322: double, V323: double, V324: double, V325: double, V326: double, V327: double, V328: double, V329: double, V330: double, V331: double, V332: double, V333: double, V334: double, V335: double, V336: double, V337: double, V338: double, V339: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_csv_to_iceberg(\"/data/train_transaction.csv\", \"hive_cat.raw.train_transaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "411fbb67-f512-46df-9d08-7215848c1b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[TransactionID: int, id_01: double, id_02: double, id_03: double, id_04: double, id_05: double, id_06: double, id_07: double, id_08: double, id_09: double, id_10: double, id_11: double, id_12: string, id_13: double, id_14: double, id_15: string, id_16: string, id_17: double, id_18: double, id_19: double, id_20: double, id_21: double, id_22: double, id_23: string, id_24: double, id_25: double, id_26: double, id_27: string, id_28: string, id_29: string, id_30: string, id_31: string, id_32: double, id_33: string, id_34: string, id_35: string, id_36: string, id_37: string, id_38: string, DeviceType: string, DeviceInfo: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_csv_to_iceberg(\"/data/train_identity.csv\", \"hive_cat.raw.train_identity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fecac1-f9a9-477c-bc98-824456c5a6bb",
   "metadata": {},
   "source": [
    "# Запись из raw в cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a10323f-be1a-473a-b0c7-37574e0ec256",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"64\")\n",
    "spark.conf.set(\"spark.sql.files.maxRecordsPerFile\", \"500000\")\n",
    "#spark.conf.set(\"parquet.enable.dictionary\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73ddcc2f-a296-4cc2-88ad-64faf766a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b8eb378-1a64-484c-b118-f00e7f1fe8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_to_cleaned(spark: SparkSession) -> None:\n",
    "\n",
    "    train_txn = spark.table(\"hive_cat.raw.train_transaction\")\n",
    "    train_id = spark.table(\"hive_cat.raw.train_identity\")\n",
    "    test_txn = spark.table(\"hive_cat.raw.test_transaction\")\n",
    "    test_id = spark.table(\"hive_cat.raw.test_identity\")\n",
    "\n",
    "    base_epoch = int(dt.datetime(2017, 1, 1, 0, 0, 0).timestamp())\n",
    "\n",
    "    def build_view(df_txn, df_id, dataset_label: str, has_target: bool):\n",
    "        df = (df_txn.alias(\"t\")\n",
    "            .join(df_id.alias(\"i\"), on=\"TransactionID\", how=\"left\")\n",
    "            .withColumn(\n",
    "                \"timestamp\",\n",
    "                F.to_timestamp(F.col(\"TransactionDT\").cast(\"long\") + F.lit(base_epoch))\n",
    "            )\n",
    "            .withColumnRenamed(\"TransactionID\", \"transaction_id\")\n",
    "            .withColumnRenamed(\"TransactionAmt\", \"amount\")\n",
    "        )\n",
    "\n",
    "        if has_target and \"isFraud\" in df.columns:\n",
    "            df = df.withColumnRenamed(\"isFraud\", \"is_fraud\")\n",
    "        else:\n",
    "            df = df.withColumn(\"is_fraud\", F.lit(None).cast(\"int\"))\n",
    "\n",
    "        df = (df\n",
    "            .withColumn(\"customer_id\", F.col(\"card1\").cast(\"string\"))\n",
    "            .withColumn(\n",
    "                \"card_id\",\n",
    "                F.concat_ws(\n",
    "                    \"-\",\n",
    "                    F.col(\"card1\").cast(\"string\"),\n",
    "                    F.col(\"card2\").cast(\"string\"),\n",
    "                    F.col(\"card3\").cast(\"string\"),\n",
    "                )\n",
    "            )\n",
    "            .withColumn(\"country\", F.col(\"addr2\").cast(\"string\"))\n",
    "            .withColumn(\"region\", F.col(\"addr1\").cast(\"string\"))\n",
    "            .withColumn(\"device_id\", F.coalesce(F.col(\"DeviceInfo\"), F.lit(\"unknown\")))\n",
    "            .withColumn(\n",
    "                \"channel\",\n",
    "                F.when(F.col(\"DeviceType\") == \"mobile\", \"MOBILE\")\n",
    "                 .when(F.col(\"DeviceType\") == \"desktop\", \"WEB\")\n",
    "                 .otherwise(\"UNKNOWN\")\n",
    "            )\n",
    "            .withColumn(\"email\", F.col(\"P_emaildomain\"))\n",
    "            .withColumn(\"dataset\", F.lit(dataset_label))\n",
    "            .withColumn(\"event_date\", F.to_date(F.col(\"timestamp\")))\n",
    "        )\n",
    "\n",
    "        important_cols = [\n",
    "            \"transaction_id\",\n",
    "            \"timestamp\",\n",
    "            \"event_date\",\n",
    "            \"amount\",\n",
    "            \"customer_id\",\n",
    "            \"card_id\",\n",
    "            \"country\",\n",
    "            \"region\",\n",
    "            \"device_id\",\n",
    "            \"channel\",\n",
    "            \"email\",\n",
    "            \"is_fraud\",\n",
    "            \"dataset\",\n",
    "        ]\n",
    "\n",
    "        cols_to_keep = [c for c in important_cols if c in df.columns]\n",
    "        df = df.select(*cols_to_keep)\n",
    "\n",
    "        return df\n",
    "\n",
    "    train_clean = build_view(train_txn, train_id, dataset_label=\"train\", has_target=True)\n",
    "    test_clean  = build_view(test_txn,  test_id,  dataset_label=\"test\",  has_target=False)\n",
    "\n",
    "    #spark.sql(\"CREATE NAMESPACE IF NOT EXISTS hive_cat.cleaned\")\n",
    "\n",
    "    (\n",
    "        train_clean\n",
    "        .repartition(\"event_date\")  # repartition(64, \"event_date\")\n",
    "        .writeTo(\"hive_cat.cleaned.transactions\")\n",
    "        .using(\"iceberg\")\n",
    "        .tableProperty(\"write.format.default\", \"parquet\")\n",
    "        .tableProperty(\"format-version\", \"2\")\n",
    "        .partitionedBy(\"event_date\")\n",
    "        .createOrReplace()\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        test_clean\n",
    "        .repartition(\"event_date\")\n",
    "        .writeTo(\"hive_cat.cleaned.transactions\")\n",
    "        .append()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5873dec2-0a17-42c8-a73d-5235d52656f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    load_raw_to_cleaned(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1111ee3-f9de-449b-9697-f07c58ad535d",
   "metadata": {},
   "source": [
    "# Запись из cleaned в слой features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e0e1a56-6ea2-4ee1-a0e2-5289d097ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e7aa7ce-fc27-4788-a465-5ef140113c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(spark: SparkSession) -> None:\n",
    "    df = spark.table(\"hive_cat.cleaned.transactions\")\n",
    "\n",
    "    df = df.filter(F.col(\"customer_id\").isNotNull())\n",
    "\n",
    "    ts_long = F.col(\"timestamp\").cast(\"long\")\n",
    "\n",
    "    w_cust_1d = (\n",
    "        Window.partitionBy(\"customer_id\")\n",
    "        .orderBy(ts_long)\n",
    "        .rangeBetween(-86400, 0)\n",
    "    )\n",
    "\n",
    "    w_cust_7d = (\n",
    "        Window.partitionBy(\"customer_id\")\n",
    "        .orderBy(ts_long)\n",
    "        .rangeBetween(-86400 * 7, 0)\n",
    "    )\n",
    "\n",
    "    w_cust_order = (\n",
    "        Window.partitionBy(\"customer_id\")\n",
    "        .orderBy(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    df_feat = (\n",
    "        df\n",
    "        .withColumn(\"hour\", F.hour(\"timestamp\"))\n",
    "        .withColumn(\n",
    "            \"is_night\",\n",
    "            F.when((F.col(\"hour\") < 6) | (F.col(\"hour\") >= 23), F.lit(1)).otherwise(F.lit(0))\n",
    "        )\n",
    "        .withColumn(\"log_amount\", F.log1p(F.col(\"amount\")))\n",
    "        .withColumn(\"cust_txn_cnt_1d\", F.count(\"*\").over(w_cust_1d))\n",
    "        .withColumn(\"cust_amt_sum_1d\", F.sum(\"amount\").over(w_cust_1d))\n",
    "        .withColumn(\"cust_txn_cnt_7d\", F.count(\"*\").over(w_cust_7d))\n",
    "        .withColumn(\"cust_amt_sum_7d\", F.sum(\"amount\").over(w_cust_7d))\n",
    "        .withColumn(\"prev_timestamp\", F.lag(\"timestamp\").over(w_cust_order))\n",
    "        .withColumn(\n",
    "            \"secs_since_prev_txn\",\n",
    "            (F.col(\"timestamp\").cast(\"long\") - F.col(\"prev_timestamp\").cast(\"long\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cols = [\n",
    "        \"transaction_id\",\n",
    "        \"timestamp\",\n",
    "        \"event_date\",\n",
    "        \"dataset\",\n",
    "        \"customer_id\",\n",
    "        \"card_id\",\n",
    "        \"country\",\n",
    "        \"region\",\n",
    "        \"device_id\",\n",
    "        \"channel\",\n",
    "        \"email\",\n",
    "        \"is_fraud\",\n",
    "        \"amount\",\n",
    "        \"log_amount\",\n",
    "        \"hour\",\n",
    "        \"is_night\",\n",
    "        \"cust_txn_cnt_1d\",\n",
    "        \"cust_amt_sum_1d\",\n",
    "        \"cust_txn_cnt_7d\",\n",
    "        \"cust_amt_sum_7d\",\n",
    "        \"secs_since_prev_txn\",\n",
    "    ]\n",
    "\n",
    "    df_feat = df_feat.select(*cols)\n",
    "\n",
    "    (\n",
    "        df_feat\n",
    "        .repartition(\"event_date\")\n",
    "        .writeTo(\"hive_cat.features.transaction_features\")\n",
    "        .using(\"iceberg\")\n",
    "        .tableProperty(\"write.format.default\", \"parquet\")\n",
    "        .tableProperty(\"format-version\", \"2\")\n",
    "        .partitionedBy(\"event_date\")\n",
    "        .createOrReplace()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e074e2ac-e59c-4363-bf8e-b30af4ed6081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    build_features(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bbe924-4b2f-4808-8b64-15fa247562d7",
   "metadata": {},
   "source": [
    "# Данные для модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42baf312-97a2-45a8-b67c-27f0ea0abf8c",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d049e2-2012-4e39-b0a3-8d8178de4ab8",
   "metadata": {},
   "source": [
    "### export train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27da6d94-5b0c-4110-b4f7-0f61f74e4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bc5c579-7fc0-49f3-b173-65c6fdb54564",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BASE = \"s3a://warehouse/features.db/lgbm_data\"\n",
    "\n",
    "def export_train_dataset(spark: SparkSession) -> None:\n",
    "    df = spark.table(\"hive_cat.features.trans_gnn\")\n",
    "\n",
    "    df_train = df.filter(\n",
    "        (F.col(\"dataset\") == \"train\") & F.col(\"is_fraud\").isNotNull()\n",
    "    )\n",
    "\n",
    "    output_path = f\"{S3_BASE}/train\"\n",
    "\n",
    "    (\n",
    "        df_train\n",
    "        # .repartition(1)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(output_path)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94cafd9c-744b-4300-9f27-e6ac52319ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    export_train_dataset(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd418c6-5f10-4d0b-812e-7e346ea8e14b",
   "metadata": {},
   "source": [
    "### export test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b76eafc-87de-4bd3-8532-a774aa608c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BASE = \"s3a://warehouse/features.db/lgbm_data\"\n",
    "\n",
    "def export_test_dataset(spark: SparkSession) -> None:\n",
    "    df = spark.table(\"hive_cat.features.trans_gnn\")\n",
    "\n",
    "    df_test = df.filter(F.col(\"dataset\") == \"test\")\n",
    "\n",
    "    output_path = f\"{S3_BASE}/test\"\n",
    "\n",
    "    (\n",
    "        df_test\n",
    "        # .repartition(1)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(output_path)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88a99835-c23e-4857-9174-34a7495f7e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    export_test_dataset(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53649781-ffc2-4dbd-965a-09eefbe7659d",
   "metadata": {},
   "source": [
    "## GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547514e-f49c-4ade-b132-1664cb0ddb49",
   "metadata": {},
   "source": [
    "### Подготовка ребер и узлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4279f55-da40-4e92-b91d-fad74771ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02c344e7-b00a-4223-856a-c25b3f205b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_gnn_graph(spark: SparkSession,src_table: str = \"hive_cat.features.transaction_features\",ns: str = \"hive_cat.graph\") -> None:\n",
    "    spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {ns}\")\n",
    "\n",
    "    df = spark.table(src_table)\n",
    "\n",
    "    df_all = (\n",
    "        df\n",
    "        .withColumn(\"transaction_id_str\", F.col(\"transaction_id\").cast(\"string\"))\n",
    "        .withColumn(\"customer_id_str\",   F.col(\"customer_id\").cast(\"string\"))\n",
    "        .withColumn(\"card_id_str\",       F.col(\"card_id\").cast(\"string\"))\n",
    "        .withColumn(\"device_id_str\",     F.col(\"device_id\").cast(\"string\"))\n",
    "        .withColumn(\"email_str\",         F.col(\"email\").cast(\"string\"))\n",
    "    )\n",
    "\n",
    "    df_train = (\n",
    "        df_all\n",
    "        .filter(F.col(\"dataset\") == \"train\")\n",
    "        .withColumn(\"label\", F.col(\"is_fraud\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    def write_iceberg(df_out, table_name: str, mode: str = \"createOrReplace\"):\n",
    "        full = f\"{ns}.{table_name}\"\n",
    "        w = df_out.writeTo(full).using(\"iceberg\") \\\n",
    "            .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "            .tableProperty(\"format-version\", \"2\")\n",
    "        if mode == \"append\":\n",
    "            w.append()\n",
    "        else:\n",
    "            w.createOrReplace()\n",
    "\n",
    "    # ---------- nodes ----------\n",
    "    customers = (\n",
    "        df_all.select(F.col(\"customer_id_str\").alias(\"customer_id\"))\n",
    "        .where(F.col(\"customer_id_str\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    write_iceberg(customers, \"nodes_customers\")\n",
    "\n",
    "    cards = (\n",
    "        df_all.select(F.col(\"card_id_str\").alias(\"card_id\"))\n",
    "        .where(F.col(\"card_id_str\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    write_iceberg(cards, \"nodes_cards\")\n",
    "\n",
    "    devices = (\n",
    "        df_all.select(F.col(\"device_id_str\").alias(\"device_id\"))\n",
    "        .where(F.col(\"device_id_str\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    write_iceberg(devices, \"nodes_devices\")\n",
    "\n",
    "    emails = (\n",
    "        df_all.select(F.col(\"email_str\").alias(\"email\"))\n",
    "        .where(F.col(\"email_str\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    write_iceberg(emails, \"nodes_emails\")\n",
    "\n",
    "    txns = (\n",
    "        df_all.select(F.col(\"transaction_id_str\").alias(\"transaction_id\"))\n",
    "        .where(F.col(\"transaction_id_str\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    write_iceberg(txns, \"nodes_transactions\")\n",
    "\n",
    "    # ---------- labales (only train) ----------\n",
    "    txn_labels = (\n",
    "        df_train\n",
    "        .select(\n",
    "            F.col(\"transaction_id_str\").alias(\"transaction_id\"),\n",
    "            F.col(\"label\"),\n",
    "        )\n",
    "        .where(F.col(\"transaction_id_str\").isNotNull())\n",
    "    )\n",
    "    write_iceberg(txn_labels, \"labels_transactions\")\n",
    "\n",
    "    # ---------- edges ----------\n",
    "    edges_cust_card = (\n",
    "        df_all.select(\n",
    "            F.col(\"customer_id_str\").alias(\"customer_id\"),\n",
    "            F.col(\"card_id_str\").alias(\"card_id\"),\n",
    "        )\n",
    "        .where(F.col(\"customer_id_str\").isNotNull() & F.col(\"card_id_str\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    write_iceberg(edges_cust_card, \"edges_customer_card\")\n",
    "    edges_card_device = (\n",
    "        df_all.select(\n",
    "            F.col(\"card_id_str\").alias(\"card_id\"),\n",
    "            F.col(\"device_id_str\").alias(\"device_id\"),\n",
    "        )\n",
    "        .where(F.col(\"card_id_str\").isNotNull() & F.col(\"device_id_str\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    write_iceberg(edges_card_device, \"edges_card_device\")\n",
    "\n",
    "    edges_card_email = (\n",
    "        df_all.select(\n",
    "            F.col(\"card_id_str\").alias(\"card_id\"),\n",
    "            F.col(\"email_str\").alias(\"email\"),\n",
    "        )\n",
    "        .where(F.col(\"card_id_str\").isNotNull() & F.col(\"email_str\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    write_iceberg(edges_card_email, \"edges_card_email\")\n",
    "\n",
    "    edges_cust_txn = (\n",
    "        df_all.select(\n",
    "            F.col(\"customer_id_str\").alias(\"customer_id\"),\n",
    "            F.col(\"transaction_id_str\").alias(\"transaction_id\"),\n",
    "        )\n",
    "        .where(F.col(\"customer_id_str\").isNotNull() & F.col(\"transaction_id_str\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    write_iceberg(edges_cust_txn, \"edges_customer_txn\")\n",
    "\n",
    "    edges_txn_device = (\n",
    "        df_all.select(\n",
    "            F.col(\"transaction_id_str\").alias(\"transaction_id\"),\n",
    "            F.col(\"device_id_str\").alias(\"device_id\"),\n",
    "        )\n",
    "        .where(F.col(\"transaction_id_str\").isNotNull() & F.col(\"device_id_str\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    write_iceberg(edges_txn_device, \"edges_txn_device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0dba698-8640-4702-8987-8de3f88700d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN graph export finished -> namespace hive_cat.graph\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    export_gnn_graph(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad80c3f-0c1b-405c-a8ed-aca981001c00",
   "metadata": {},
   "source": [
    "### import/join gnn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95679356-bdc0-498b-9728-a911fa656d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17145109-62dc-450b-857e-203729ef4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_gnn_to_features(\n",
    "    spark: SparkSession,\n",
    "    gnn_path: str = \"s3a://warehouse/features.db/gnn_features/txn_gnn_features.parquet\",) -> None:\n",
    "    df_gnn = spark.read.parquet(gnn_path)\n",
    "\n",
    "    df_gnn = df_gnn.withColumn(\"transaction_id_str\", F.col(\"transaction_id\").cast(\"string\")) \\\n",
    "                   .drop(\"transaction_id\")\n",
    "\n",
    "    df_feat = spark.table(\"hive_cat.features.transaction_features\") \\\n",
    "                   .withColumn(\"transaction_id_str\", F.col(\"transaction_id\").cast(\"string\"))\n",
    "\n",
    "    matched = df_feat.join(df_gnn, on=\"transaction_id_str\", how=\"inner\").count()\n",
    "    #print(\"Matched rows by transaction_id_str:\", matched)\n",
    "\n",
    "    df_join = (\n",
    "        df_feat.join(df_gnn, on=\"transaction_id_str\", how=\"left\")\n",
    "               .drop(\"transaction_id_str\")\n",
    "    )\n",
    "\n",
    "    fill_map = {\"gnn_score\": 0.0}\n",
    "    for i in range(1, 17):\n",
    "        col = f\"gnn_emb_{i}\"\n",
    "        if col in df_join.columns:\n",
    "            fill_map[col] = 0.0\n",
    "\n",
    "    df_join = df_join.na.fill(fill_map)\n",
    "\n",
    "    (\n",
    "        df_join\n",
    "        .repartition(\"event_date\")\n",
    "        .writeTo(\"hive_cat.features.trans_gnn\")\n",
    "        .using(\"iceberg\")\n",
    "        .tableProperty(\"write.format.default\", \"parquet\")\n",
    "        .tableProperty(\"format-version\", \"2\")\n",
    "        .partitionedBy(\"event_date\")\n",
    "        .createOrReplace()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "412e18ed-ec05-4306-91a5-c2dc5fefe74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched rows by transaction_id_str: 1097231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 13:11:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import_gnn_to_features(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd60b644-9f44-4636-95c6-1ff2c6bb516c",
   "metadata": {},
   "source": [
    "# Запись в marts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7abcc756-9898-46aa-b7eb-4f3660aa4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b58f54c1-577d-4e09-980e-8298b8bb8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_scored_to_marts(\n",
    "    spark: SparkSession,\n",
    "    scored_path: str = \"s3a://warehouse/features.db/lgbm_data/scored/test_scored.parquet\",) -> None:\n",
    "    df_scored = spark.read.parquet(scored_path)\n",
    "    \n",
    "    df_scored = (\n",
    "        df_scored\n",
    "        .withColumn(\"event_date\", F.to_date(F.col(\"event_date\")))\n",
    "        .withColumn(\"fraud_score\", F.col(\"fraud_score\").cast(\"double\"))\n",
    "        .withColumn(\"fraud_label\", F.col(\"fraud_label\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    spark.sql(\"CREATE NAMESPACE IF NOT EXISTS hive_cat.marts\")\n",
    "\n",
    "    (\n",
    "        df_scored\n",
    "        .repartition(\"event_date\")\n",
    "        .writeTo(\"hive_cat.marts.scored_transactions\")\n",
    "        .using(\"iceberg\")\n",
    "        .tableProperty(\"write.format.default\", \"parquet\")\n",
    "        .tableProperty(\"format-version\", \"2\")\n",
    "        .partitionedBy(\"event_date\")\n",
    "        .createOrReplace()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da199c40-8cad-4ad2-8601-76ce23ceb875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import_scored_to_marts(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdec377-007e-402a-8ec6-b2b3d7ac16bd",
   "metadata": {},
   "source": [
    "# Кластеризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17b91922-e5a0-4ea8-af94-d5ad4422cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9262ead8-3042-430a-9859-5934b4309946",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"hive_cat.marts.scored_transactions\") \\\n",
    "    .select(\n",
    "        \"transaction_id\",\n",
    "        \"log_amount\",\n",
    "        F.log10(F.col(\"secs_since_prev_txn\") + 1).alias(\"log_secs_prev\"),\n",
    "        F.log10(F.col(\"cust_txn_cnt_7d\") + 1).alias(\"log_txn_cnt_7d\"),\n",
    "        \"hour\",\n",
    "        \"is_night\",\n",
    "        \"fraud_score\",\n",
    "        \"fraud_label\"\n",
    "    ) \\\n",
    "    .na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96625c6d-59e7-41e8-8a5e-8bac69aa214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"log_amount\", \"log_secs_prev\", \"log_txn_cnt_7d\", \"hour\", \"is_night\"],\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "df2 = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "779f3402-124d-46bd-98cd-3c85875a7e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "model_scaler = scaler.fit(df2)\n",
    "df3 = model_scaler.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab45d7a4-4980-4f83-bf00-e99501edf758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 14:55:58 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "k = 6\n",
    "kmeans = BisectingKMeans(k=k, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "model_kmeans = kmeans.fit(df3)\n",
    "\n",
    "df_clusters = model_kmeans.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d4a914d-0eba-49c2-839b-e13998eefa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_clusters\n",
    "    .select(\n",
    "        \"transaction_id\",\n",
    "        \"cluster\",\n",
    "        \"log_amount\",\n",
    "        \"log_secs_prev\",\n",
    "        \"log_txn_cnt_7d\",\n",
    "        \"hour\",\n",
    "        \"is_night\",\n",
    "        \"fraud_score\",\n",
    "        \"fraud_label\"\n",
    "    )\n",
    "    .writeTo(\"hive_cat.marts.scored_transactions_clustered\")\n",
    "    .using(\"iceberg\")\n",
    "    .createOrReplace()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (container)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
